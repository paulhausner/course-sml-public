{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {"tags": []}, "outputs": [], "source": "import pandas as pd\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimport sklearn.linear_model as skl_lm\n\nfrom matplotlib.patches import Arc\n\nimport itertools\nimport math\n\nmpl.rcParams['axes.spines.right'] = False\nmpl.rcParams['axes.spines.top'] = False"}, {"cell_type": "markdown", "metadata": {"tags": []}, "source": "# 9.1 Analysing happiness across countries"}, {"cell_type": "markdown", "metadata": {"tags": []}, "source": "In this exercise, we will consider the data set `data/happy.csv` with data from the World Happiness Report. For details see: https://worldhappiness.report/ed/2019/changing-world-happiness/"}, {"cell_type": "markdown", "metadata": {"tags": []}, "source": "## Dataset\n\n`GDP per capita` is in terms of Purchasing Power Parity (PPP) adjusted to constant 2011 international dollars, taken from the World Development Indicators (WDI) released by the World Bank on November 14, 2018. The equation uses the natural log of GDP per capita, as this form fits the data significantly better than GDP per capita.\n\nThe time series of `healthy life expectancy` at birth are constructed based on data from the World Health Organization (WHO) Global Health Observatory data repository, with data available for 2005, 2010, 2015, and 2016. To match this report\u2019s sample period, interpolation and extrapolation are used. \n\n`Social support` is the national average of the binary responses (either 0 or 1) to the Gallup World Poll (GWP) question \u201cIf you were in trouble, do you have relatives or friends you can count on to help you whenever you need them, or not?\u201d\n\n`Freedom to make life choices` is the national average of binary responses to the GWP question \u201cAre you satisfied or dissatisfied with your freedom to choose what you do with your life?\u201d\n\n`Generosity` is the residual of regressing the national average of GWP responses to the question \u201cHave you donated money to a charity in the past month?\u201d on GDP per capita.\n\n`Perceptions of corruption` are the average of binary answers to two GWP questions: \u201cIs corruption widespread throughout the government or not?\u201d and \u201cIs corruption widespread within businesses or not?\u201d Where data for government corruption are missing, the perception of business corruption is used as the overall corruption-perception measure."}, {"cell_type": "markdown", "metadata": {"tags": []}, "source": "## a)\n\nLoad and familiarize yourself with the data set."}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": false, "tags": []}, "outputs": [], "source": "# Read in the data\n#happy = pd.read_csv(\"data/happy.csv\",delimiter=';') \nhappy = pd.read_csv('https://uu-sml.github.io/course-sml-public/data/happy.csv', delimiter=';')\nhappy.head()"}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": []}, "outputs": [], "source": "# Rename some columns\nhappy.rename(columns = {\n    'Perceptions of corruption':'Corruption',\n    'Log GDP per capita': 'LogGDP',\n    'Healthy life expectancy at birth': 'LifeExp',\n    'Freedom to make life choices': 'Freedom',\n}, inplace = True) \n\n# In this exercise we will just analyse one year. 2017.\ndf = happy[happy['Year'] == 2017].dropna()\ndf.head()"}, {"cell_type": "markdown", "metadata": {"tags": []}, "source": "## b)\nThe code below fits a linear regression model to predict life ladder (happiness) as a function of social support. Edit the code to fit a third order polynomial. What model would you suggest to use?"}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": []}, "outputs": [], "source": "#Fit model.\nX_train = df[['Social support']]\ny_train = df['Life Ladder']\nmodel = skl_lm.LinearRegression(fit_intercept=False)\nmodel.fit(X_train, y_train)\n\n# Print the solution\nprint(f'The coefficient is: {model.coef_[0]:.3f}')"}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": []}, "outputs": [], "source": "#Compute predictions.\nx = np.arange(0.25, 1, step=0.01)\nX_test = x.reshape(-1, 1)\ny_test = model.predict(X_test)"}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": []}, "outputs": [], "source": "#Fit model polynomial\n"}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": []}, "outputs": [], "source": "#Compute predictions.\n"}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": []}, "outputs": [], "source": "# Plot social support and life ladder data\nplt.plot('Social support', 'Life Ladder', 'o', data=df, color='gray')\ncountries = ['United Kingdom','Croatia', 'Benin', 'Finland',\n             'Afghanistan']\nfor country in countries:\n    ci = np.where(df['Country name'] == country)[0][0]\n    plt.plot(df.iloc[ci]['Social support'],\n             df.iloc[ci]['Life Ladder'], 'ko')\n    plt.annotate(country,\n                 xy=(df.iloc[ci]['Social support'],\n                     df.iloc[ci]['Life Ladder']),\n                 xytext=(3, 3),  # 3 points offset\n                 textcoords=\"offset points\",\n                 ha='left', va='bottom')\n\n# Plot model\nplt.plot(x, y_test, 'k')\nplt.ylabel('Life Satisfaction (y)')\nplt.xlabel('Social support (s)')\nplt.show()"}, {"cell_type": "markdown", "metadata": {"tags": []}, "source": "## c)\n\nThe code below fits a linear regression model to predict life ladder (happiness) as a linear function of six variables. Use AIC as a manual tool to investigate what the best model is combining these factors.\n\nThe AIC of a model is defined as\n$$\n\\mathrm{AIC} = 2 k - 2 \\ell,\n$$\nwhere $k$ is the number of model parameters and $\\ell$ is the maximum log-likelihood of the model. In the case of a linear regression model\n$$\ny = \\theta_0 + \\theta_1 x_1 + \\cdots + \\theta_p x_p + \\epsilon, \\qquad \\epsilon \\sim \\mathcal{N}(0, \\sigma^2)\n$$\nwith intercept the number of parameters is $k = p + 2$ ($\\theta_0, \\ldots, \\theta_p$ and $\\sigma$). Without intercept,\nthe number of parameters would be $k = p + 1$ ($\\theta_1, \\ldots, \\theta_p$ and $\\sigma$)."}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": []}, "outputs": [], "source": "factors = ['LogGDP', 'Social support', 'LifeExp',  'Freedom',\n           'Generosity', 'Corruption']\n\n# Fit regression model\nX = df[factors]\ny = df['Life Ladder']\nmodel = skl_lm.LinearRegression()\nmodel.fit(X, y)\n\n# Print the solution\nprint('The coefficients are:', model.coef_)\nprint(f'The offset is: {model.intercept_:.3f}')\n\n# Compute predictions\ny_hat = model.predict(X)\n\n# Compute AIC\ndef aic(model, y, y_hat):\n    # Numbers of parameters of linear regression model\n    # Variance of Gaussian noise is a parameter as well!\n    k = model.coef_.size + model.get_params()['fit_intercept'] + 1\n    \n    # Compute maximum log-likelihood\n    n = y.size\n    mse = np.mean((y - y_hat)**2)\n    loglik = - n / 2 * (1 + math.log(2 * math.pi) + np.log(mse))\n    \n    return 2 * (k - loglik)\n\nprint(f'The AIC is: {aic(model, y, y_hat):.3f}')"}, {"cell_type": "markdown", "metadata": {"tags": []}, "source": "## d)\n\nWrite an automated code to find the model with the smallest AIC."}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": false, "tags": []}, "outputs": [], "source": ""}, {"cell_type": "markdown", "metadata": {"tags": []}, "source": "This can very likely be improved on."}, {"cell_type": "markdown", "metadata": {"tags": []}, "source": "# 9.2 Analysing goals in football"}, {"cell_type": "markdown", "metadata": {"tags": []}, "source": "In this exercise, we will consider the data set `data/shots.csv`. This is a collection of all shots and goals in the English premier league for one season. See: https://figshare.com/articles/dataset/Events/7770599"}, {"cell_type": "markdown", "metadata": {"tags": []}, "source": "## Data\n'Goal' 1 if a goal, 0 if not a goal\n'X' x-location along long side of pitch in co-ordinates (0-100)\n'Y' y-location along short side of pitch (where goal is) in co-ordinates (0-100)\n'Distance' is distance (in metres) from middle of goal.\n'Angle' is of a triangle created fom the shot point to the goal mouth (as descibed in lectures)."}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": []}, "outputs": [], "source": "#Load data for all shots\n#shots_model=pd.read_csv('data/shots.csv')\nshots_model = pd.read_csv('https://uu-sml.github.io/course-sml-public/data/shots.csv')\nshots_model.head()"}, {"cell_type": "markdown", "metadata": {"tags": []}, "source": "Function for plotting goal mouth "}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": []}, "outputs": [], "source": "def createGoalMouth():\n    #Adopted from FC Python\n    #Create figure\n    plt.figure()\n    ax = plt.gca()\n\n    linecolor='black'\n\n    #Pitch Outline & Centre Line\n    plt.plot([0,65],[0,0], color=linecolor)\n    plt.plot([65,65],[50,0], color=linecolor)\n    plt.plot([0,0],[50,0], color=linecolor)\n    \n    #Left Penalty Area\n    plt.plot([12.5,52.5],[16.5,16.5],color=linecolor)\n    plt.plot([52.5,52.5],[16.5,0],color=linecolor)\n    plt.plot([12.5,12.5],[0,16.5],color=linecolor)\n    \n    #Left 6-yard Box\n    plt.plot([41.5,41.5],[5.5,0],color=linecolor)\n    plt.plot([23.5,41.5],[5.5,5.5],color=linecolor)\n    plt.plot([23.5,23.5],[0,5.5],color=linecolor)\n    \n    #Goal\n    plt.plot([41.5-5.34,41.5-5.34],[-2,0],color=linecolor)\n    plt.plot([23.5+5.34,41.5-5.34],[-2,-2],color=linecolor)\n    plt.plot([23.5+5.34,23.5+5.34],[0,-2],color=linecolor)\n    \n    #Prepare Circles\n    leftPenSpot = plt.Circle((65/2,11),0.8,color=linecolor)\n    \n    #Draw Circles\n    ax.add_patch(leftPenSpot)\n    \n    #Prepare Arcs\n    leftArc = Arc((32.5,11),height=18.3,width=18.3,angle=0,theta1=38,theta2=142,color=linecolor)\n    \n    #Draw Arcs\n    ax.add_patch(leftArc)\n    \n    #Set limits\n    plt.xlim(-1,66)\n    plt.ylim(-3,35)\n\n    #Tidy Axes\n    plt.axis('off')\n    \n    #Set layout\n    plt.tight_layout()\n    ax.set_aspect('equal', adjustable='box')"}, {"cell_type": "markdown", "metadata": {"tags": []}, "source": "## a)\nThe code plot the frequency of the data. "}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": []}, "outputs": [], "source": "# Compute a two-dimensional histogram of shots from different points\nshotcounts, _, _ = np.histogram2d(shots_model['X'], shots_model['Y'],\n                                  bins=50, range=[[0, 100],[0, 100]])\n\n# Plot the number of shots from different points\ncreateGoalMouth()\npos = plt.imshow(shotcounts, extent=[-1,66,104,-1], cmap=plt.cm.Reds)\nplt.colorbar(pos)\nplt.title('Number of shots')\nplt.show()"}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": []}, "outputs": [], "source": "# Compute a two-dimensional histogram of goals from different points\ngoals_only = shots_model[shots_model['Goal'] == 1]\ngoalcounts, _, _ = np.histogram2d(goals_only['X'], goals_only['Y'],\n                                  bins=50, range=[[0, 100],[0, 100]])\n\n# Plot the number of goals from different points\ncreateGoalMouth()\npos = plt.imshow(goalcounts, extent=[-1,66,104,-1], cmap=plt.cm.Reds)\nplt.colorbar(pos)\nplt.title('Number of goals')\nplt.show()"}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": []}, "outputs": [], "source": "# Compute empirical probability of scoring from different points\nwith np.errstate(divide='ignore', invalid='ignore'):\n    prob_goal = goalcounts / shotcounts\n\n# Plot the probability of scoring from different points.\ncreateGoalMouth()\npos = plt.imshow(prob_goal, extent=[-1,66,104,-1], cmap=plt.cm.Reds,\n                 vmin=0, vmax=0.5)\nplt.colorbar(pos)\nplt.title('Proportion of shots resulting in a goal')\nplt.show()"}, {"cell_type": "markdown", "metadata": {"tags": []}, "source": "## b)\nThe code below plots how shot angle determine probability of scoring. It fits a logistic regression model and compares it to data. Make a similar plot for distance to goal. See what happens when you add distance squared."}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": []}, "outputs": [], "source": "# Make single variable model of angle\n# Using logistic regression we find the optimal parameters\nX = shots_model[['Angle']]\ny = shots_model['Goal']\nmodel = skl_lm.LogisticRegression(penalty='none')\nmodel.fit(X, y)\n\n# Bin the angles of shots to compute empirical estimates of the\n# probabilities of goals scored\nshotcount_angle, bin_edges = np.histogram(shots_model['Angle'] * 180 / np.pi, bins=40, range=[0, 150])\ngoalcount_angle, _ = np.histogram(goals_only['Angle'] * 180 / np.pi, bins=40, range=[0, 150])\n\n# Compute average angle in each bin\nangle = (bin_edges[:-1] + bin_edges[1:])/2\n\n# Empirical estimate of probabilities of goal scored\n# for bins with at least one shot\nibins = np.where(shotcount_angle > 0)\nprob_goal = goalcount_angle[ibins] / shotcount_angle[ibins]\n\n# Compute predictions\nxGprob = model.predict_proba(angle.reshape(-1, 1) * np.pi / 180) \n\n# Plot data and predictions\nplt.plot(angle[ibins], prob_goal, 'ko')\nplt.plot(angle, xGprob[:,1], 'k')\nplt.xlabel(\"Shot angle (degrees)\")\nplt.ylabel('Probability chance scored')\nplt.show()"}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": []}, "outputs": [], "source": "#Show empirically how distance from goal predicts probability of scoring\n#Make single variable model of distance\n"}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": []}, "outputs": [], "source": "#Adding distance squared\n"}, {"cell_type": "markdown", "metadata": {"tags": []}, "source": "## c)\nBy setting `model_variables` in the code below you can test different features. Investigate manually which parameters work best. "}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": []}, "outputs": [], "source": "# Adding even more variables to the model.\nshots_model['X2'] = shots_model['X']**2\nshots_model['C2'] = shots_model['C']**2\nshots_model['AX'] = shots_model['Angle']*shots_model['X']\n\n# A general model for fitting goal probability\n# List the model variables you want here\nmodel_variables = ['Distance']\n\n# Fit the linear regression model.\nX = shots_model[model_variables]\ny = shots_model['Goal']\nmodel = skl_lm.LogisticRegression(penalty='none')\nmodel.fit(X, y)\n\n# Number of parameters of logistic regression model\nk = model.coef_.size + model.get_params()['fit_intercept']\nprint(f'The number of parameters is: {k:d}')\n    \n# Compute maximum log-likelihood\nn = y.size\nloglik = np.sum(np.log(model.predict_proba(X)[np.arange(n), y]))\nprint(f'The log-likelihood is: {loglik:.3f}')\n\n# Compute AIC\nAIC = 2 * (k - loglik)\nprint(f'The AIC is: {AIC:.3f}')\n\n# Create a 2D map of predicted probabilities\npgoal_2d = np.zeros((65,65))\nfor x in range(65):\n    for y in range(65):\n        # Compute features for this field\n        sh = dict()\n        a = np.arctan(7.32 *x /(x**2 + abs(y-65/2)**2 - (7.32/2)**2))\n        if a < 0:\n            a = np.pi + a\n        sh['Angle'] = a\n        sh['Distance'] = np.sqrt(x**2 + abs(y-65/2)**2)\n        sh['D2'] = x**2 + abs(y-65/2)**2\n        sh['X'] = x\n        sh['AX'] = x*a\n        sh['X2'] = x**2\n        sh['C'] = abs(y-65/2)\n        sh['C2'] = (y-65/2)**2\n        \n        # Compute predictions\n        X_field = np.array([sh[var] for var in model_variables]).reshape(1, -1)\n        pgoal_2d[x, y] = model.predict_proba(X_field)[:, 1]\n\n# Plot model\ncreateGoalMouth()\nplt.imshow(pgoal_2d, extent=[-1,65,65,-1],\n           cmap=plt.cm.Reds, vmin=0, vmax=0.3)\nplt.colorbar(pos)\nplt.title('Probability of goal')\nplt.show()"}], "metadata": {"@webio": {"lastCommId": null, "lastKernelId": null}, "celltoolbar": "Tags", "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.6"}}, "nbformat": 4, "nbformat_minor": 2}